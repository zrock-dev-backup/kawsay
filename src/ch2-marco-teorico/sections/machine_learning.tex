\subsection{Machine Learning}
El Machine Learning (ML), es una rama de la inteligencia artificial que se enfoca en el desarrollo de sistemas capaces de aprender y mejorar a partir de la experiencia, sin ser explícitamente programados para cada tarea específica \parencite{Samuel1959}.
En el contexto de este proyecto, el ML se aplica para analizar datos históricos académicos y predecir la elegibilidad de los estudiantes para inscribirse en futuras asignaturas, buscando optimizar la asignación de cupos y la planificación académica, abordando así uno de los objetivos principales de entender el impacto del ML en este proceso.

\paragraph{Aprendizaje no supervisado}
trabaja con datos no etiquetados\footnote{Los \textbf{datos no etiquetados} en machine learning son datos de entrada que no tienen una salida o categoría predefinida asociada.}, buscando descubrir patrones, estructuras o relaciones inherentes en la información sin una guía previa sobre las salidas correctas \parencite{Hastie2009}.
Técnicas comunes incluyen el clustering\footnote{El \textbf{clustering} (agrupamiento) es una técnica de aprendizaje no supervisado que consiste en agrupar un conjunto de objetos de tal manera que los objetos en el mismo grupo (llamado clúster) sean más similares entre sí que con los de otros grupos.} (agrupación) y la reducción de dimensionalidad\footnote{La \textbf{reducción de dimensionalidad} es el proceso de reducir el número de variables (o características) consideradas en un conjunto de datos, mientras se conserva la información importante.
Se utiliza para simplificar modelos, mejorar el rendimiento y facilitar la visualización.}.
Aunque el enfoque principal del proyecto es supervisado para la predicción de elegibilidad, el análisis exploratorio de datos podría emplear técnicas no supervisadas para identificar grupos de estudiantes con perfiles académicos similares o detectar anomalías\footnote{Las \textbf{anomalías} (o valores atípicos) son puntos de datos que difieren significativamente de otras observaciones.
En el contexto de datos de inscripción, podrían ser patrones inusuales que merecen investigación.} en los patrones de inscripción, complementando la comprensión del proceso actual.

\paragraph{Aprendizaje supervisado}
es un paradigma del ML donde el algoritmo aprende a partir de un conjunto de datos previamente etiquetado\footnote{Los \textbf{datos etiquetados} en machine learning son datos de entrada que ya tienen una salida o categoría correcta conocida asociada.
El modelo aprende a mapear las entradas a las salidas correctas.}, es decir, cada ejemplo de entrada está asociado a una salida correcta conocida \parencite{Bishop2006}.
El objetivo es entrenar un modelo que pueda predecir la etiqueta de salida para nuevas entradas no vistas.
Para la predicción de elegibilidad de estudiantes en Jala University, se utilizarán datos históricos (calificaciones, cursos aprobados, plan de estudios) como entradas etiquetadas (elegible/no elegible para un curso específico) para entrenar un modelo predictivo, como podría ser una regresión logística\footnote{La \textbf{regresión logística} es un algoritmo de aprendizaje supervisado utilizado para problemas de clasificación binaria (predecir una de dos categorías).
Modela la probabilidad de que una instancia pertenezca a una clase particular.} o una máquina de soporte vectorial.

La Figura~\ref{fig:mlComparison} representa cómo un modelo no supervisado utiliza datos no etiquetados para la clasificación y el aprendizaje supervisado utiliza datos etiquetados.

\begin{figure}
    \centering
    \caption{Representación esquemática de los paradigmas de aprendizaje, } \label{fig:mlComparison}
    \includegraphics[width=0.45\textwidth]{comparison-supervised-unsupervised.pdf}

    \vspace{0.5em}
    \begin{minipage}{\textwidth}
        \small\textit{Nota.} Fuente: \textcite{morimoto2021}.
    \end{minipage}
\end{figure}

Los Árboles de Decisión y los Bosques Aleatorios son dos modelos que se ajustan a los requisitos del proyecto.

\subsubsection{Árboles de Decisión}
Un árbol de decisión es un algoritmo de aprendizaje supervisado utilizado tanto para tareas de clasificación como de regresión\footnote{La \textbf{regresión} en machine learning es un tipo de tarea de aprendizaje supervisado donde el objetivo es predecir un valor numérico continuo (por ejemplo, el precio de una casa, la temperatura).
La \textbf{clasificación}, por otro lado, predice una etiqueta categórica (por ejemplo, si un correo es spam o no).}.
Opera particionando recursivamente el espacio de características\footnote{El \textbf{espacio de características} (feature space) es el espacio n-dimensional definido por las características (variables) utilizadas para describir las instancias de datos.
Cada instancia de datos es un punto en este espacio.} en regiones distintas basadas en una serie de reglas de decisión inferidas de los datos \parencite{quinlan1986induction}.
Cada nodo interno en el árbol representa una prueba sobre un atributo (característica), cada rama representa el resultado de la prueba, y cada nodo hoja representa una etiqueta de clase (para clasificación) o una predicción (para regresión).

El proceso de aprendizaje del árbol de decisión implica seleccionar las características más informativas para dividir los datos en cada nodo, con el objetivo de crear subconjuntos cada vez más homogéneos.
Este proceso continúa recursivamente hasta que se cumple un criterio de parada, como alcanzar una profundidad máxima del árbol, un número mínimo de muestras en un nodo, o lograr una clasificación (o predicción) perfecta dentro de un nodo.

\paragraph{Criterios de División}
La selección de la mejor característica para realizar la división es crucial en la construcción de árboles de decisión.
Los criterios de división comunes incluyen:

\begin{itemize}
    \item \textbf{Impureza de Gini (para clasificación):} Mide la impureza de un conjunto de instancias, donde valores más bajos indican una mayor homogeneidad.
Es una medida de cuán a menudo un elemento elegido al azar del conjunto sería incorrectamente etiquetado si fuera etiquetado aleatoriamente según la distribución de etiquetas en el subconjunto.
    \item \textbf{Ganancia de Información (para clasificación):} Mide la reducción de la entropía\footnote{La \textbf{entropía} en teoría de la información es una medida de la incertidumbre o impureza en un conjunto de datos.
En los árboles de decisión, se utiliza para cuantificar la homogeneidad de las etiquetas de clase en un nodo.} después de dividir por un atributo.
    \item \textbf{Error Cuadrático Medio (ECM) (para regresión):} Mide la diferencia cuadrática promedio entre los valores predichos y los reales.
\end{itemize}

\subsubsection{Bosques Aleatorios}
El algoritmo de Bosques Aleatorios (Random Forest), introducido por Breiman \cite{breiman2001random}, es un algoritmo de aprendizaje supervisado.
Es un conjunto (ensemble)\footnote{Un \textbf{método de ensamble} (ensemble method) en machine learning combina las predicciones de múltiples modelos base (como árboles de decisión) para producir una predicción final mejor y más robusta que la de cualquier modelo individual.} de árboles de decisión, donde cada árbol se entrena con un subconjunto aleatorio de los datos y un subconjunto aleatorio de las características.
La predicción final se obtiene agregando las predicciones de todos los árboles individuales.
Para tareas de clasificación, esto se hace típicamente por votación mayoritaria\footnote{La \textbf{votación mayoritaria} en un ensamble de clasificadores significa que la clase predicha es la que recibe más "votos" (predicciones) de los modelos individuales que componen el ensamble.}, mientras que para tareas de regresión, se utiliza la predicción promedio.

El algoritmo de Bosques Aleatorios aprovecha el principio de la "sabiduría de la multitud", combinando las diversas perspectivas de múltiples árboles de decisión para producir predicciones más precisas y estables que las que pueden proporcionar los árboles individuales.
Los pasos fundamentales involucrados en la construcción de un Bosque Aleatorio son:

\begin{enumerate}
    \item \textbf{Muestreo Bootstrap (Bootstrap Sampling):} El algoritmo selecciona aleatoriamente, con reemplazo, \textit{n} muestras del conjunto de datos de entrenamiento original para crear un conjunto de entrenamiento único para cada árbol.
    Este proceso, denominado bootstrapping, asegura que cada árbol se entrene con un subconjunto de datos ligeramente diferente.
    \item \textbf{Aleatoriedad de Características (Feature Randomness):} Para cada nodo dentro de un árbol, se elige un subconjunto aleatorio de \textit{m} características del total de \textit{p} características disponibles.
    Luego, el algoritmo identifica la división óptima para el nodo basándose en estas \textit{m} características.
    El parámetro \textit{m} es típicamente significativamente más pequeño que \textit{p}, un paso crucial para reducir la correlación\footnote{La \textbf{correlación} entre árboles en un bosque aleatorio se refiere a la tendencia de diferentes árboles a cometer errores similares en las mismas instancias.
Reducir esta correlación es clave para la efectividad del ensamble.} entre los árboles.
    \item \textbf{Crecimiento del Árbol (Tree Growth):} Cada árbol se cultiva hasta su máxima profundidad sin poda\footnote{La \textbf{poda} (pruning) en árboles de decisión es una técnica para reducir el tamaño del árbol eliminando secciones que proporcionan poco poder predictivo, con el fin de evitar el sobreajuste (overfitting).} (o hasta que se alcanza un tamaño mínimo de nodo predefinido).
    Esto permite que cada árbol capture relaciones complejas en sus datos de entrenamiento.
    \item \textbf{Agregación de Predicciones (Prediction Aggregation):} Para problemas de clasificación, la etiqueta de clase final se determina mediante votación mayoritaria entre todos los árboles.
    En escenarios de regresión, la predicción final es el promedio de las predicciones generadas por los árboles individuales.
\end{enumerate}

\paragraph{Ajuste de Hiperparámetros}
es una fase crítica en la optimización del rendimiento de un modelo de Bosques Aleatorios.
Los hiperparámetros clave que impactan significativamente el comportamiento del modelo incluyen:

\begin{itemize}
    \item \texttt{n\_estimators}: El número de árboles dentro del bosque.
    \item \texttt{max\_features}: El número de características consideradas al buscar la mejor división en cada nodo.
    \item \texttt{max\_depth}: La profundidad máxima permitida de los árboles individuales.
    \item \texttt{min\_samples\_split}: El número mínimo de muestras requerido para dividir un nodo interno.
    \item \texttt{min\_samples\_leaf}: El número mínimo de muestras obligatorio para residir en un nodo hoja.
\end{itemize}

Dos metodologías ampliamente utilizadas para el ajuste de hiperparámetros son la Búsqueda en Rejilla (Grid Search) y la Búsqueda Aleatoria (Random Search).

\begin{itemize}
	\item \textbf{Búsqueda en Rejilla (Grid Search):} es una técnica de búsqueda exhaustiva que evalúa sistemáticamente todas las combinaciones posibles de valores de hiperparámetros dentro de una rejilla predefinida \cite{bergstra2012random}.
	El usuario especifica un conjunto discreto de valores para cada hiperparámetro, y la Búsqueda en Rejilla explora meticulosamente cada combinación.

	\item \textbf{Búsqueda Aleatoria (Random Search):} es un método de búsqueda estocástica\footnote{Un \textbf{método estocástico} es aquel que incorpora algún elemento de aleatoriedad en su proceso.
	En la búsqueda aleatoria de hiperparámetros, las combinaciones se eligen al azar.} que muestrea aleatoriamente combinaciones de hiperparámetros de distribuciones especificadas \cite{bergstra2012random}.
	A diferencia de la exploración exhaustiva de la Búsqueda en Rejilla, la Búsqueda Aleatoria sondea un subconjunto aleatorio del espacio de hiperparámetros.
\end{itemize}

\paragraph{Conclusión:}
Para este proyecto vamos a utilizar Bosques Aleatorios.
